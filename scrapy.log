2026-01-07 08:15:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'NodeScrapy.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2026-01-07 08:15:54 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py:44: ScrapyDeprecationWarning: RandomUserAgentMiddleware.process_request() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(mw.process_request)

2026-01-07 08:15:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2026-01-07 08:15:54 [scrapy.middleware] INFO: Enabled item pipelines:
['NodeScrapy.pipelines.Pipeline']
2026-01-07 08:15:54 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:41: ScrapyDeprecationWarning: Pipeline.open_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.open_spider)

2026-01-07 08:15:54 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:44: ScrapyDeprecationWarning: Pipeline.close_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.close_spider)

2026-01-07 08:15:54 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:47: ScrapyDeprecationWarning: Pipeline.process_item() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.process_item)

2026-01-07 08:15:54 [scrapy.core.engine] INFO: Spider opened
2026-01-07 08:15:54 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py:490: ScrapyDeprecationWarning: NodeScrapy.spiders.SimpleSpider.SimpleSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2026-01-07 08:15:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2026-01-07 08:15:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2026-01-07 08:15:54 [simple] INFO: clashmeta start
2026-01-07 08:15:54 [simple] INFO: ndnode start
2026-01-07 08:15:54 [simple] INFO: nodev2ray start
2026-01-07 08:15:54 [simple] INFO: nodefree start
2026-01-07 08:15:54 [simple] INFO: v2rayshare start
2026-01-07 08:15:54 [simple] INFO: wenode start
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-7-clash-meta-node.htm on 2026-01-07
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-7-clash-meta-node.htm on 2026-01-07
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-6-clash-meta-github.htm on 2026-01-06
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-6-clash-meta-github.htm on 2026-01-06
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-5-free-node-subscribe.htm on 2026-01-05
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-5-free-node-subscribe.htm on 2026-01-05
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-4-clash-meta-node-github.htm on 2026-01-04
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-4-clash-meta-node-github.htm on 2026-01-04
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-3-clash-meta-node.htm on 2026-01-03
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-3-clash-meta-node.htm on 2026-01-03
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-2-free-node-subscribe-links.htm on 2026-01-02
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-2-free-node-subscribe-links.htm on 2026-01-02
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-1-clash-meta-windows.htm on 2026-01-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2026-1-1-clash-meta-windows.htm on 2026-01-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-31-clash-meta-node-share.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-27-clash-meta-node-share.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-26-node-share-links.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-25-free-high-speed-nodes.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-24-clash-meta-windows.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-23-clash-meta-node-github.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-22-node-share.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-21-free-high-speed-nodes.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-20-free-clash-meta-node.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-19-node-share.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-18-clash-meta-windows.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-17-clash-meta-node-share.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta found /free-nodes/2025-12-16-node-share.htm on 2026-03-01
2026-01-07 08:15:54 [simple] INFO: clashmeta is up to date, exiting
2026-01-07 08:15:54 [simple] INFO: clashmeta is up to date, exiting
2026-01-07 08:15:54 [simple] INFO: clashmeta is up to date, exiting
2026-01-07 08:15:54 [simple] INFO: ndnode needs update, accessing https://www.naidounode.com/
2026-01-07 08:15:54 [simple] INFO: ndnode needs update, accessing https://www.naidounode.com/freenode
2026-01-07 08:15:54 [simple] INFO: ndnode needs update, accessing https://www.naidounode.com
2026-01-07 08:15:55 [simple] INFO: nodev2ray found /free-node/2026-1-7-free-shadowrocket-node.htm on 2026-01-07
2026-01-07 08:15:55 [simple] INFO: nodev2ray found /free-node/2026-1-6-free-subscribe-node.htm on 2026-01-06
2026-01-07 08:15:55 [simple] INFO: nodev2ray found /free-node/2026-1-5-v2ray-windows.htm on 2026-01-05
2026-01-07 08:15:55 [simple] INFO: nodev2ray found /free-node/2026-1-4-v2ray-node.htm on 2026-01-04
2026-01-07 08:15:55 [simple] INFO: nodev2ray found /free-node/2026-1-3-free-ssr-subscribe.htm on 2026-01-03
2026-01-07 08:15:55 [simple] INFO: nodev2ray found /free-node/2026-1-2-v2ray-windows.htm on 2026-01-02
2026-01-07 08:15:55 [simple] INFO: nodev2ray found /free-node/2026-1-1-v2ray-node.htm on 2026-01-01
2026-01-07 08:15:55 [simple] INFO: nodev2ray is up to date, exiting
2026-01-07 08:15:55 [simple] INFO: nodev2ray is up to date, exiting
2026-01-07 08:15:55 [simple] INFO: nodev2ray is up to date, exiting
2026-01-07 08:15:55 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5316.html on 2026-01-07
2026-01-07 08:15:55 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5317.html on 2026-01-06
2026-01-07 08:15:55 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5318.html on 2026-01-05
2026-01-07 08:15:55 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5319.html on 2026-01-04
2026-01-07 08:15:55 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5320.html on 2026-01-03
2026-01-07 08:15:55 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5321.html on 2026-01-02
2026-01-07 08:15:55 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5322.html on 2026-01-01
2026-01-07 08:15:55 [simple] INFO: v2rayshare is up to date, exiting
2026-01-07 08:15:55 [simple] INFO: v2rayshare is up to date, exiting
2026-01-07 08:15:55 [simple] INFO: v2rayshare is up to date, exiting
2026-01-07 08:15:55 [simple] INFO: nodefree found https://nodefree.me/p/3913.html on 2026-01-07
2026-01-07 08:15:55 [simple] INFO: nodefree found https://nodefree.me/p/3914.html on 2026-01-06
2026-01-07 08:15:55 [simple] INFO: nodefree found https://nodefree.me/p/3915.html on 2026-01-05
2026-01-07 08:15:55 [simple] INFO: nodefree found https://nodefree.me/p/3916.html on 2026-01-04
2026-01-07 08:15:55 [simple] INFO: nodefree found https://nodefree.me/p/3917.html on 2026-01-03
2026-01-07 08:15:55 [simple] INFO: nodefree found https://nodefree.me/p/3918.html on 2026-01-02
2026-01-07 08:15:55 [simple] INFO: nodefree found https://nodefree.me/p/3919.html on 2026-01-01
2026-01-07 08:15:55 [simple] INFO: nodefree needs update, accessing https://nodefree.me/p/3913.html
2026-01-07 08:15:55 [simple] INFO: nodefree is up to date, exiting
2026-01-07 08:15:55 [simple] INFO: nodefree is up to date, exiting
2026-01-07 08:15:56 [simple] INFO: nodefree found https://nodefree.githubrowcontent.com/2026/01/20260107.txt
2026-01-07 08:15:56 [simple] INFO: nodefree found https://nodefree.githubrowcontent.com/2026/01/20260107.yaml
2026-01-07 08:15:56 [simple] INFO: Pipeline processing nodefree.txt
2026-01-07 08:15:56 [simple] INFO: Pipeline processed nodefree.txt
2026-01-07 08:15:56 [simple] INFO: Pipeline processing nodefree.yaml
2026-01-07 08:15:56 [simple] INFO: Pipeline processed nodefree.yaml
2026-01-07 08:15:56 [scrapy.core.engine] INFO: Closing spider (finished)
2026-01-07 08:15:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 9453,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 139589,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 15,
 'downloader/response_status_count/301': 9,
 'downloader/response_status_count/404': 5,
 'dupefilter/filtered': 2,
 'elapsed_time_seconds': 2.362005,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2026, 1, 7, 8, 15, 56, 679980, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 603693,
 'httpcompression/response_count': 18,
 'item_scraped_count': 2,
 'items_per_minute': 60.0,
 'log_count/INFO': 78,
 'memusage/max': 240492544,
 'memusage/startup': 240492544,
 'request_depth_max': 2,
 'response_received_count': 20,
 'responses_per_minute': 600.0,
 'robotstxt/request_count': 10,
 'robotstxt/response_count': 10,
 'robotstxt/response_status_count/200': 5,
 'robotstxt/response_status_count/404': 5,
 'scheduler/dequeued': 13,
 'scheduler/dequeued/memory': 13,
 'scheduler/enqueued': 13,
 'scheduler/enqueued/memory': 13,
 'start_time': datetime.datetime(2026, 1, 7, 8, 15, 54, 317975, tzinfo=datetime.timezone.utc)}
2026-01-07 08:15:56 [scrapy.core.engine] INFO: Spider closed (finished)
2026-01-07 08:16:02 [scrapy.addons] INFO: Enabled addons:
[]
2026-01-07 08:16:02 [scrapy.extensions.telnet] INFO: Telnet Password: b9143488b69b8a51
2026-01-07 08:16:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logcount.LogCount',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2026-01-07 08:16:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'NodeScrapy',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'NodeScrapy.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['NodeScrapy.spiders']}
2026-01-07 08:16:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'NodeScrapy.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2026-01-07 08:16:03 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/downloader/middleware.py:44: ScrapyDeprecationWarning: RandomUserAgentMiddleware.process_request() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(mw.process_request)

2026-01-07 08:16:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2026-01-07 08:16:03 [scrapy.middleware] INFO: Enabled item pipelines:
['NodeScrapy.pipelines.Pipeline']
2026-01-07 08:16:03 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:41: ScrapyDeprecationWarning: Pipeline.open_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.open_spider)

2026-01-07 08:16:03 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:44: ScrapyDeprecationWarning: Pipeline.close_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.close_spider)

2026-01-07 08:16:03 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/pipelines/__init__.py:47: ScrapyDeprecationWarning: Pipeline.process_item() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.process_item)

2026-01-07 08:16:03 [scrapy.core.engine] INFO: Spider opened
2026-01-07 08:16:03 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py:490: ScrapyDeprecationWarning: NodeScrapy.spiders.SimpleSpider.SimpleSpider (inherited by NodeScrapy.spiders.DecryptSpider.DecryptSpider) defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2026-01-07 08:16:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2026-01-07 08:16:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2026-01-07 08:16:03 [decrypt] INFO: yudou66 start
2026-01-07 08:16:03 [decrypt] INFO: blues start
2026-01-07 08:16:03 [decrypt] INFO: blues found https://blues2022.blogspot.com/2025/09/2025964kchatgpt4k8kclashv2ray.html on 2026-09-06
2026-01-07 08:16:03 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2025/12/202512294kchatgpt4k8kclashv2ray.html
2026-01-07 08:16:03 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2025/12/202512284kchatgpt4k8kclashv2ray.html
2026-01-07 08:16:03 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2025/12/202512274kchatgpt4k8kclashv2ray.html
2026-01-07 08:16:03 [decrypt] INFO: blues found yt_url: https://youtu.be/BCDTvi9__u8
2026-01-07 08:16:03 [pytubefix.__main__] WARNING: `use_po_token` and `po_token_verifier` is deprecated and will be removed soon.
2026-01-07 08:16:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://blues2022.blogspot.com/2025/12/202512294kchatgpt4k8kclashv2ray.html?m=1> (referer: https://blues2022.blogspot.com/?m=1)
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/defer.py", line 355, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/python.py", line 311, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/python.py", line 311, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 61, in process_spider_output
    yield from super().process_spider_output(response, result)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/home/runner/work/FreeNodes/FreeNodes/NodeScrapy/spiders/DecryptSpider.py", line 76, in parse_blog
    pwdfinder = PwdFinder(name, self.logger, yt_url)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/FreeNodes/FreeNodes/utils/PwdFinder.py", line 65, in __init__
    match = re.search(r"(?:\d{4}[-年])?(\d{1,2})[-月](\d{1,2})", yt.title)
                                                                 ^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 815, in title
    self._author = self.vid_info.get("videoDetails", {}).get(
                   ^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 494, in vid_info
    self._vid_info = self.vid_info_client()
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 538, in vid_info_client
    innertube_response = call_innertube(optional_client)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 531, in call_innertube
    response = innertube.player(self.video_id)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 877, in player
    return self._call_api(endpoint, query, self.base_data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 771, in _call_api
    self.fetch_bearer_token()
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 687, in fetch_bearer_token
    self.oauth_verifier(verification_url, user_code)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 521, in _default_oauth_verifier
    input('Press enter when you have completed this step.')
EOFError: EOF when reading a line
2026-01-07 08:16:04 [decrypt] INFO: blues found yt_url: https://youtu.be/JGmltwFwjyM
2026-01-07 08:16:04 [pytubefix.__main__] WARNING: `use_po_token` and `po_token_verifier` is deprecated and will be removed soon.
2026-01-07 08:16:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://blues2022.blogspot.com/2025/12/202512274kchatgpt4k8kclashv2ray.html> (referer: https://blues2022.blogspot.com/?m=1)
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/defer.py", line 355, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/python.py", line 311, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/python.py", line 311, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 61, in process_spider_output
    yield from super().process_spider_output(response, result)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/home/runner/work/FreeNodes/FreeNodes/NodeScrapy/spiders/DecryptSpider.py", line 76, in parse_blog
    pwdfinder = PwdFinder(name, self.logger, yt_url)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/FreeNodes/FreeNodes/utils/PwdFinder.py", line 65, in __init__
    match = re.search(r"(?:\d{4}[-年])?(\d{1,2})[-月](\d{1,2})", yt.title)
                                                                 ^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 815, in title
    self._author = self.vid_info.get("videoDetails", {}).get(
                   ^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 494, in vid_info
    self._vid_info = self.vid_info_client()
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 538, in vid_info_client
    innertube_response = call_innertube(optional_client)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 531, in call_innertube
    response = innertube.player(self.video_id)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 877, in player
    return self._call_api(endpoint, query, self.base_data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 771, in _call_api
    self.fetch_bearer_token()
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 687, in fetch_bearer_token
    self.oauth_verifier(verification_url, user_code)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 521, in _default_oauth_verifier
    input('Press enter when you have completed this step.')
EOFError: EOF when reading a line
2026-01-07 08:16:05 [decrypt] INFO: blues found yt_url: https://youtu.be/WZsQFIO0U3U
2026-01-07 08:16:05 [pytubefix.__main__] WARNING: `use_po_token` and `po_token_verifier` is deprecated and will be removed soon.
2026-01-07 08:16:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://blues2022.blogspot.com/2025/12/202512284kchatgpt4k8kclashv2ray.html> (referer: https://blues2022.blogspot.com/?m=1)
Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/defer.py", line 355, in iter_errback
    yield next(it)
          ^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/python.py", line 311, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/utils/python.py", line 311, in __next__
    return next(self.data)
           ^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/depth.py", line 61, in process_spider_output
    yield from super().process_spider_output(response, result)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/spidermiddlewares/base.py", line 60, in process_spider_output
    for o in result:
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 183, in process_sync
    self._process_spider_exception(
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 256, in _process_spider_exception
    raise exception
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/scrapy/core/spidermw.py", line 179, in process_sync
    yield from iterable
  File "/home/runner/work/FreeNodes/FreeNodes/NodeScrapy/spiders/DecryptSpider.py", line 76, in parse_blog
    pwdfinder = PwdFinder(name, self.logger, yt_url)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/FreeNodes/FreeNodes/utils/PwdFinder.py", line 65, in __init__
    match = re.search(r"(?:\d{4}[-年])?(\d{1,2})[-月](\d{1,2})", yt.title)
                                                                 ^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 815, in title
    self._author = self.vid_info.get("videoDetails", {}).get(
                   ^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 494, in vid_info
    self._vid_info = self.vid_info_client()
                     ^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 538, in vid_info_client
    innertube_response = call_innertube(optional_client)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/__main__.py", line 531, in call_innertube
    response = innertube.player(self.video_id)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 877, in player
    return self._call_api(endpoint, query, self.base_data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 771, in _call_api
    self.fetch_bearer_token()
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 687, in fetch_bearer_token
    self.oauth_verifier(verification_url, user_code)
  File "/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytubefix/innertube.py", line 521, in _default_oauth_verifier
    input('Press enter when you have completed this step.')
EOFError: EOF when reading a line
2026-01-07 08:16:06 [scrapy.core.engine] INFO: Closing spider (finished)
2026-01-07 08:16:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 3539,
 'downloader/request_count': 10,
 'downloader/request_method_count/GET': 10,
 'downloader/response_bytes': 169790,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 6,
 'downloader/response_status_count/302': 3,
 'elapsed_time_seconds': 3.998833,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2026, 1, 7, 8, 16, 7, 41839, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 974014,
 'httpcompression/response_count': 6,
 'items_per_minute': 0.0,
 'log_count/ERROR': 3,
 'log_count/INFO': 12,
 'log_count/WARNING': 3,
 'memusage/max': 240181248,
 'memusage/startup': 240181248,
 'request_depth_max': 1,
 'response_received_count': 6,
 'responses_per_minute': 120.0,
 'retry/count': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'robotstxt/forbidden': 1,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 2,
 'scheduler/dequeued': 7,
 'scheduler/dequeued/memory': 7,
 'scheduler/enqueued': 7,
 'scheduler/enqueued/memory': 7,
 'spider_exceptions/EOFError': 3,
 'spider_exceptions/count': 3,
 'start_time': datetime.datetime(2026, 1, 7, 8, 16, 3, 43006, tzinfo=datetime.timezone.utc)}
2026-01-07 08:16:07 [scrapy.core.engine] INFO: Spider closed (finished)
